{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Networks (CapsNets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper: [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829), by Sara Sabour, Nicholas Frosst and Geoffrey E. Hinton (NIPS 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired in part from Huadong Liao's implementation: [CapsNet-TensorFlow](https://github.com/naturomics/CapsNet-Tensorflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add convolutional layers before capsules to get low level features (still getting 6\\*6 feature maps) **DONE**\n",
    "- Reduce batch size **DONE**\n",
    "- Change validation set from last 104 (they may be all from one class) to a random sample **DONE**\n",
    "- *Normalize* images **DONE**\n",
    "- Reduce *learning rate* **DONE** from `learning_rate=.001` (default) to `learning_rate=.00001`\n",
    "- Look into initialization/unnecessary *regularization*\n",
    "- Increase number of capsules\n",
    "- Change to plain CNN\n",
    "- Change Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support both Python 2 and Python 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot pretty figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need NumPy and TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reset the default graph, in case you re-run this notebook without restarting the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the random seeds so that this notebook always produces the same output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('../data/processed/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train.is_iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.read_json('../data/processed/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the corresponding labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a Capsule Network to classify these images. Here's the overall architecture, enjoy the ASCII art! ;-)\n",
    "Note: for readability, I left out two arrows: Labels → Mask, and Input Images → Reconstruction Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "                            Loss\n",
    "                              ↑\n",
    "                    ┌─────────┴─────────┐\n",
    "      Labels → Margin Loss      Reconstruction Loss\n",
    "                    ↑                   ↑\n",
    "                  Length             Decoder\n",
    "                    ↑                   ↑ \n",
    "             Digit Capsules ────Mask────┘\n",
    "               ↖↑↗ ↖↑↗ ↖↑↗\n",
    "             Primary Capsules\n",
    "                    ↑      \n",
    "               Input Images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build the graph starting from the bottom layer, and gradually move up, left side first. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a placeholder for the input images (28×28 pixels, 1 color channel = grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, 75, 75, 1], dtype=tf.float32, name=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary Capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer will be composed of 32 maps of 6×6 capsules each, where each capsule will output an 8D activation vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute their outputs, we first apply two regular convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_params = {\n",
    "    \"filters\": 64, #was 256\n",
    "    \"kernel_size\": 5,\n",
    "    \"strides\": 1, #was 1\n",
    "    \"padding\": \"same\", #was valid\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv11_params = {\n",
    "    \"filters\": 64, #was 256\n",
    "    \"kernel_size\": 3, #was 5\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"same\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv12_params = { #additional layer\n",
    "    \"filters\": 128, #was 256\n",
    "    \"kernel_size\": 3, #was 5\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"same\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv13_params = { #additional layer\n",
    "    \"filters\": 128,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 3,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv2_params = {\n",
    "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "    \"kernel_size\": 6, #was 9\n",
    "    \"strides\": 3, #was 2\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
    "conv11 = tf.layers.conv2d(conv1, name=\"conv11\", **conv11_params)\n",
    "conv12 = tf.layers.conv2d(conv11, name=\"conv12\", **conv12_params)\n",
    "conv13 = tf.layers.conv2d(conv12, name=\"conv13\", **conv13_params)\n",
    "conv2 = tf.layers.conv2d(conv13, name=\"conv2\", **conv2_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we used a kernel size of 9 and no padding (for some reason, that's what `\"valid\"` means), the image shrunk by 9-1=8 pixels after each convolutional layer (28×28 to 20×20, then 20×20 to 12×12), and since we used a stride of 2 in the second convolutional layer, the image size was divided by 2. This is how we end up with 6×6 feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we reshape the output to get a bunch of 8D vectors representing the outputs of the primary capsules. The output of `conv2` is an array containing 32×8=256 feature maps for each instance, where each feature map is 6×6. So the shape of this output is (_batch size_, 6, 6, 256). We want to chop the 256 into 32 vectors of 8 dimensions each. We could do this by reshaping to (_batch size_, 6, 6, 32, 8). However, since this first capsule layer will be fully connected to the next capsule layer, we can simply flatten the 6×6 grids. This means we just need to reshape to (_batch size_, 6×6×32, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],\n",
    "                       name=\"caps1_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to squash these vectors. Let's define the `squash()` function, based on equation (1) from the paper:\n",
    "\n",
    "$\\operatorname{squash}(\\mathbf{s}) = \\dfrac{\\|\\mathbf{s}\\|^2}{1 + \\|\\mathbf{s}\\|^2} \\dfrac{\\mathbf{s}}{\\|\\mathbf{s}\\|}$\n",
    "\n",
    "The `squash()` function will squash all vectors in the given array, along the given axis (by default, the last axis).\n",
    "\n",
    "**Caution**, a nasty bug is waiting to bite you: the derivative of $\\|\\mathbf{s}\\|$ is undefined when $\\|\\mathbf{s}\\|=0$, so we can't just use `tf.norm()`, or else it will blow up during training: if a vector is zero, the gradients will be `nan`, so when the optimizer updates the variables, they will also become `nan`, and from then on you will be stuck in `nan` land. The solution is to implement the norm manually by computing the square root of the sum of squares plus a tiny epsilon value: $\\|\\mathbf{s}\\| \\approx \\sqrt{\\sum\\limits_i{{s_i}^2}\\,\\,+ \\epsilon}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this function to get the output $\\mathbf{u}_i$ of each primary capsules $i$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps1_output = squash(caps1_raw, name=\"caps1_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have the output of the first capsule layer. It wasn't too hard, was it? However, computing the next layer is where the fun really begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the output of the digit capsules, we must first compute the predicted output vectors (one for each primary / digit capsule pair). Then we can run the routing by agreement algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Predicted Output Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digit capsule layer contains 10 capsules (one for each digit) of 16 dimensions each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_n_caps = 2\n",
    "caps2_n_dims = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each capsule $i$ in the first layer, we want to predict the output of every capsule $j$ in the second layer. For this, we will need a transformation matrix $\\mathbf{W}_{i,j}$ (one for each pair of capsules ($i$, $j$)), then we can compute the predicted output $\\hat{\\mathbf{u}}_{j|i} = \\mathbf{W}_{i,j} \\, \\mathbf{u}_i$ (equation (2)-right in the paper). Since we want to transform an 8D vector into a 16D vector, each transformation matrix $\\mathbf{W}_{i,j}$ must have a shape of (16, 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $\\hat{\\mathbf{u}}_{j|i}$ for every pair of capsules ($i$, $j$), we will use a nice feature of the `tf.matmul()` function: you probably know that it lets you multiply two matrices, but you may not know that it also lets you multiply higher dimensional arrays. It treats the arrays as arrays of matrices, and it performs itemwise matrix multiplication. For example, suppose you have two 4D arrays, each containing a 2×3 grid of matrices. The first contains matrices $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D}, \\mathbf{E}, \\mathbf{F}$ and the second contains matrices $\\mathbf{G}, \\mathbf{H}, \\mathbf{I}, \\mathbf{J}, \\mathbf{K}, \\mathbf{L}$. If you multiply these two 4D arrays using the `tf.matmul()` function, this is what you get:\n",
    "\n",
    "$\n",
    "\\pmatrix{\n",
    "\\mathbf{A} & \\mathbf{B} & \\mathbf{C} \\\\\n",
    "\\mathbf{D} & \\mathbf{E} & \\mathbf{F}\n",
    "} \\times\n",
    "\\pmatrix{\n",
    "\\mathbf{G} & \\mathbf{H} & \\mathbf{I} \\\\\n",
    "\\mathbf{J} & \\mathbf{K} & \\mathbf{L}\n",
    "} = \\pmatrix{\n",
    "\\mathbf{AG} & \\mathbf{BH} & \\mathbf{CI} \\\\\n",
    "\\mathbf{DJ} & \\mathbf{EK} & \\mathbf{FL}\n",
    "}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this function to compute $\\hat{\\mathbf{u}}_{j|i}$ for every pair of capsules ($i$, $j$) like this (recall that there are 6×6×32=1152 capsules in the first layer, and 10 in the second layer):\n",
    "\n",
    "$\n",
    "\\pmatrix{\n",
    "  \\mathbf{W}_{1,1} & \\mathbf{W}_{1,2} & \\cdots & \\mathbf{W}_{1,10} \\\\\n",
    "  \\mathbf{W}_{2,1} & \\mathbf{W}_{2,2} & \\cdots & \\mathbf{W}_{2,10} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\mathbf{W}_{1152,1} & \\mathbf{W}_{1152,2} & \\cdots & \\mathbf{W}_{1152,10}\n",
    "} \\times\n",
    "\\pmatrix{\n",
    "  \\mathbf{u}_1 & \\mathbf{u}_1 & \\cdots & \\mathbf{u}_1 \\\\\n",
    "  \\mathbf{u}_2 & \\mathbf{u}_2 & \\cdots & \\mathbf{u}_2 \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\mathbf{u}_{1152} & \\mathbf{u}_{1152} & \\cdots & \\mathbf{u}_{1152}\n",
    "}\n",
    "=\n",
    "\\pmatrix{\n",
    "\\hat{\\mathbf{u}}_{1|1} & \\hat{\\mathbf{u}}_{2|1} & \\cdots & \\hat{\\mathbf{u}}_{10|1} \\\\\n",
    "\\hat{\\mathbf{u}}_{1|2} & \\hat{\\mathbf{u}}_{2|2} & \\cdots & \\hat{\\mathbf{u}}_{10|2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\hat{\\mathbf{u}}_{1|1152} & \\hat{\\mathbf{u}}_{2|1152} & \\cdots & \\hat{\\mathbf{u}}_{10|1152}\n",
    "}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the first array is (1152, 10, 16, 8), and the shape of the second array is (1152, 10, 8, 1). Note that the second array must contain 10 identical copies of the vectors $\\mathbf{u}_1$ to $\\mathbf{u}_{1152}$. To create this array, we will use the handy `tf.tile()` function, which lets you create an array containing many copies of a base array, tiled in any way you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, wait a second! We forgot one dimension: _batch size_. Say we feed 50 images to the capsule network, it will make predictions for these 50 images simultaneously. So the shape of the first array must be (50, 1152, 10, 16, 8), and the shape of the second array must be (50, 1152, 10, 8, 1). The first layer capsules actually already output predictions for all 50 images, so the second array will be fine, but for the first array, we will need to use `tf.tile()` to have 50 copies of the transformation matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's start by creating a trainable variable of shape (1, 1152, 10, 16, 8) that will hold all the transformation matrices. The first dimension of size 1 will make this array easy to tile. We initialize this variable randomly using a normal distribution with a standard deviation to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sigma = 0.01\n",
    "\n",
    "W_init = tf.random_normal(\n",
    "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "W = tf.Variable(W_init, name=\"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the first array by repeating `W` once per instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = tf.shape(X)[0]\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! On to the second array, now. As discussed earlier, we need to create an array of shape (_batch size_, 1152, 10, 8, 1), containing the output of the first layer capsules, repeated 10 times (once per digit, along the third dimension, which is axis=2). The `caps1_output` array has a shape of (_batch size_, 1152, 8), so we first need to expand it twice, to get an array of shape (_batch size_, 1152, 1, 8, 1), then we can repeat it 10 times along the third dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the first array:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, and now the second:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Now, to get all the predicted output vectors $\\hat{\\mathbf{u}}_{j|i}$, we just need to multiply these two arrays using `tf.matmul()`, as explained earlier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                            name=\"caps2_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, for each instance in the batch (we don't know the batch size yet, hence the \"?\") and for each pair of first and second layer capsules (1152×10) we have a 16D predicted output column vector (16×1). We're ready to apply the routing by agreement algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing by agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's initialize the raw routing weights $b_{i,j}$ to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                       dtype=np.float32, name=\"raw_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see why we need the last two dimensions of size 1 in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's apply the softmax function to compute the routing weights, $\\mathbf{c}_{i} = \\operatorname{softmax}(\\mathbf{b}_i)$ (equation (3) in the paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the weighted sum of all the predicted output vectors for each second-layer capsule, $\\mathbf{s}_j = \\sum\\limits_{i}{c_{i,j}\\hat{\\mathbf{u}}_{j|i}}$ (equation (2)-left in the paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                   name=\"weighted_predictions\")\n",
    "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,\n",
    "                             name=\"weighted_sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple important details to note here:\n",
    "* To perform elementwise matrix multiplication (also called the Hadamard product, noted $\\circ$), we use the `tf.multiply()` function. It requires `routing_weights` and `caps2_predicted` to have the same rank, which is why we added two extra dimensions of size 1 to `routing_weights`, earlier.\n",
    "* The shape of `routing_weights` is (_batch size_, 1152, 10, 1, 1) while the shape of `caps2_predicted` is (_batch size_, 1152, 10, 16, 1).  Since they don't match on the fourth dimension (1 _vs_ 16), `tf.multiply()` automatically _broadcasts_ the `routing_weights` 16 times along that dimension. If you are not familiar with broadcasting, a simple example might help:\n",
    "\n",
    "  $ \\pmatrix{1 & 2 & 3 \\\\ 4 & 5 & 6} \\circ \\pmatrix{10 & 100 & 1000} = \\pmatrix{1 & 2 & 3 \\\\ 4 & 5 & 6} \\circ \\pmatrix{10 & 100 & 1000 \\\\ 10 & 100 & 1000} = \\pmatrix{10 & 200 & 3000 \\\\ 40 & 500 & 6000} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's apply the squash function to get the outputs of the second layer capsules at the end of the first iteration of the routing by agreement algorithm, $\\mathbf{v}_j = \\operatorname{squash}(\\mathbf{s}_j)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                              name=\"caps2_output_round_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! We have ten 16D output vectors for each instance, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's measure how close each predicted vector $\\hat{\\mathbf{u}}_{j|i}$ is to the actual output vector $\\mathbf{v}_j$ by computing their scalar product $\\hat{\\mathbf{u}}_{j|i} \\cdot \\mathbf{v}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quick math reminder: if $\\vec{a}$ and $\\vec{b}$ are two vectors of equal length, and $\\mathbf{a}$ and $\\mathbf{b}$ are their corresponding column vectors (i.e., matrices with a single column), then $\\mathbf{a}^T \\mathbf{b}$ (i.e., the matrix multiplication of the transpose of $\\mathbf{a}$, and $\\mathbf{b}$) is a 1×1 matrix containing the scalar product of the two vectors $\\vec{a}\\cdot\\vec{b}$. In Machine Learning, we generally represent vectors as column vectors, so when we talk about computing the scalar product $\\hat{\\mathbf{u}}_{j|i} \\cdot \\mathbf{v}_j$, this actually means computing ${\\hat{\\mathbf{u}}_{j|i}}^T \\mathbf{v}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to compute the scalar product $\\hat{\\mathbf{u}}_{j|i} \\cdot \\mathbf{v}_j$ for each instance, and for each pair of first and second level capsules $(i, j)$, we will once again take advantage of the fact that `tf.matmul()` can multiply many matrices simultaneously. This will require playing around with `tf.tile()` to get all dimensions to match (except for the last 2), just like we did earlier. So let's look at the shape of `caps2_predicted`, which holds all the predicted output vectors $\\hat{\\mathbf{u}}_{j|i}$ for each instance and each pair of capsules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get these shapes to match, we just need to tile the `caps2_output_round_1` array 1152 times (once per primary capsule) along the second dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_round_1_tiled = tf.tile(\n",
    "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_1_tiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to call `tf.matmul()` (note that we must tell it to transpose the matrices in the first array, to get ${\\hat{\\mathbf{u}}_{j|i}}^T$ instead of $\\hat{\\mathbf{u}}_{j|i}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                      transpose_a=True, name=\"agreement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update the raw routing weights $b_{i,j}$ by simply adding the scalar product $\\hat{\\mathbf{u}}_{j|i} \\cdot \\mathbf{v}_j$ we just computed: $b_{i,j} \\gets b_{i,j} + \\hat{\\mathbf{u}}_{j|i} \\cdot \\mathbf{v}_j$ (see Procedure 1, step 7, in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weights_round_2 = tf.add(raw_weights, agreement,\n",
    "                             name=\"raw_weights_round_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of round 2 is the same as in round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_2\")\n",
    "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_2\")\n",
    "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_2\")\n",
    "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could go on for a few more rounds, by repeating exactly the same steps as in round 2, but to keep things short, we will stop here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output = caps2_output_round_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimated Class Probabilities (Length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengths of the output vectors represent the class probabilities, so we could just use `tf.norm()` to compute them, but as we saw when discussing the squash function, it would be risky, so instead let's create our own `safe_norm()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the class of each instance, we can just select the one with the highest estimated probability. To do this, let's start by finding its index using `tf.argmax()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shape of `y_proba_argmax`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what we wanted: for each instance, we now have the index of the longest output vector. Let's get rid of the last two dimensions by using `tf.squeeze()` which removes dimensions of size 1. This gives us the capsule network's predicted class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we are now ready to define the training operations, starting with the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need a placeholder for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper uses a special margin loss to make it possible **to detect two or more different digits in each image**:\n",
    "\n",
    "$ L_k = T_k \\max(0, m^{+} - \\|\\mathbf{v}_k\\|)^2 + \\lambda (1 - T_k) \\max(0, \\|\\mathbf{v}_k\\| - m^{-})^2$\n",
    "\n",
    "* $T_k$ is equal to 1 if the digit of class $k$ is present, or 0 otherwise.\n",
    "* In the paper, $m^{+} = 0.9$, $m^{-} = 0.1$ and $\\lambda = 0.5$.\n",
    "* Note that there was an error in the video (at 15:47): the max operations are squared, not the norms. Sorry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `y` will contain the digit classes, from 0 to 9, to get $T_k$ for every instance and every class, we can just use the `tf.one_hot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small example should make it clear what this does:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the norm of the output vector for each output capsule and each instance. First, let's verify the shape of `caps2_output`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 16D output vectors are in the second to last dimension, so let's use the `safe_norm()` function with `axis=-2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
    "                              name=\"caps2_output_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute $\\max(0, m^{+} - \\|\\mathbf{v}_k\\|)^2$, and reshape the result to get a simple matrix of shape (_batch size_, 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                              name=\"present_error_raw\")\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 2),\n",
    "                           name=\"present_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's compute $\\max(0, \\|\\mathbf{v}_k\\| - m^{-})^2$ and reshape it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                             name=\"absent_error_raw\")\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 2),\n",
    "                          name=\"absent_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compute the loss for each instance and each digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "           name=\"L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sum the digit losses for each instance ($L_0 + L_1 + \\cdots + L_9$), and compute the mean over all instances. This gives us the final margin loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a decoder network on top of the capsule network. It is a regular 3-layer fully connected neural network which will learn to reconstruct the input images based on the output of the capsule network. This will force the capsule network to preserve all the information required to reconstruct the digits, across the whole network. This constraint regularizes the model: it reduces the risk of overfitting the training set, and it helps generalize to new digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper mentions that during training, instead of sending all the outputs of the capsule network to the decoder network, we must send only the output vector of the capsule that corresponds to the target digit. All the other output vectors must be masked out. At inference time, we must mask all output vectors except for the longest one, i.e., the one that corresponds to the predicted digit. You can see this in the paper's figure 2 (at 18:15 in the video): all output vectors are masked out, except for the reconstruction target's output vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a placeholder to tell TensorFlow whether we want to mask the output vectors based on the labels (`True`) or on the predictions (`False`, the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                               name=\"mask_with_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `tf.cond()` to define the reconstruction targets as the labels `y` if `mask_with_labels` is `True`, or `y_pred` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `tf.cond()` function expects the if-True and if-False tensors to be passed _via_ functions: these functions will be called just once during the graph construction phase (not during the execution phase), similar to `tf.while_loop()`. This allows TensorFlow to add the necessary operations to handle the conditional evaluation of the if-True or if-False tensors. However, in our case, the tensors `y` and `y_pred` are already created by the time we call `tf.cond()`, so unfortunately TensorFlow will consider both `y` and `y_pred` to be dependencies of the `reconstruction_targets` tensor. The `reconstruction_targets` tensor will end up with the correct value, but:\n",
    "1. whenever we evaluate a tensor that depends on `reconstruction_targets`, the `y_pred` tensor will be evaluated (even if `mask_with_layers` is `True`). This is not a big deal because computing `y_pred` adds no computing overhead during training, since we need it anyway to compute the margin loss. And during testing, if we are doing classification, we won't need reconstructions, so `reconstruction_targets` won't be evaluated at all.\n",
    "2. we will always need to feed a value for the `y` placeholder (even if `mask_with_layers` is `False`). This is a bit annoying, but we can pass an empty array, because TensorFlow won't use it anyway (it just does not know it yet when it checks for dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the reconstruction targets, let's create the reconstruction mask. It should be equal to 1.0 for the target class, and 0.0 for the other classes, for each instance. For this we can just use the `tf.one_hot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmh, its shape is (_batch size_, 1, 10, 16, 1). We want to multiply it by the `reconstruction_mask`, but the shape of the `reconstruction_mask` is (_batch size_, 10). We must reshape it to (_batch size_, 1, 10, 1, 1) to make multiplication possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_mask_reshaped = tf.reshape(\n",
    "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "    name=\"reconstruction_mask_reshaped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last! We can apply the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_masked = tf.multiply(\n",
    "    caps2_output, reconstruction_mask_reshaped,\n",
    "    name=\"caps2_output_masked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last reshape operation to flatten the decoder's inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.reshape(caps2_output_masked,\n",
    "                           [-1, caps2_n_caps * caps2_n_dims],\n",
    "                           name=\"decoder_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an array of shape (_batch size_, 160):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the decoder. It's quite simple: two dense (fully connected) ReLU layers followed by a dense output sigmoid layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 75 * 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the reconstruction loss. It is just the squared difference between the input image and the reconstructed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_sum(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss is the sum of the margin loss and the reconstruction loss (scaled down by a factor of 0.0005 to ensure the margin loss dominates training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0005\n",
    "\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Touches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure our model's accuracy, we need to count the number of instances that are properly classified. For this, we can simply compare `y` and `y_pred`, convert the boolean value to a float32 (0.0 for False, 1.0 for True), and compute the mean over all the instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper mentions that the authors used the Adam optimizer with TensorFlow's default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=.0001)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init and Saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's add the usual variable initializer, as well as a `Saver`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... we're done with the construction phase! Please take a moment to celebrate. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our capsule network is pretty standard. For simplicity, we won't do any fancy hyperparameter tuning, dropout or anything, we will just run the training operation over and over again, displaying the loss, and at the end of each epoch, measure the accuracy on the validation set, display it, and save the model if the validation loss is the lowest seen found so far (this is a basic way to implement early stopping, without actually stopping). Hopefully the code should be self-explanatory, but here are a few details to note:\n",
    "* if a checkpoint file exists, it will be restored (this makes it possible to interrupt training, then restart it later from the last checkpoint),\n",
    "* we must not forget to feed `mask_with_labels=True` during training,\n",
    "* during testing, we let `mask_with_labels` default to `False` (but we still feed the labels since they are required to compute the accuracy),\n",
    "* the images loaded _via_ `mnist.train.next_batch()` are represented as `float32` arrays of shape \\[784\\], but the input placeholder `X` expects a `float32` array of shape \\[28, 28, 1\\], so we must reshape the images before we feed them to our model,\n",
    "* we evaluate the model's loss and accuracy on the full validation set (5,000 instances). To view progress and support systems that don't have a lot of RAM, the code evaluates the loss and accuracy on one batch at a time, and computes the mean loss and mean accuracy at the end.\n",
    "\n",
    "*Warning*: if you don't have a GPU, training will take a very long time (at least a few hours). With a GPU, it should take just a few minutes per epoch (e.g., 6 minutes on an NVidia GeForce GTX 1080Ti)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For now we're using only *one* of the two channels/bands from the Statoil dataset pictures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_array = np.array(train['band_1'][0]).reshape(75,75)\n",
    "#img_array_norm = (img_array - img_array.min()) / (img_array.max() - img_array.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-27.878361, -27.15416 , -28.668615, ..., -25.573483, -26.488674,\n",
       "        -30.507013],\n",
       "       [-28.66853 , -27.878401, -27.508776, ..., -26.488632, -28.671562,\n",
       "        -31.594166],\n",
       "       [-28.66853 , -27.15416 , -25.865042, ..., -27.157106, -27.881393,\n",
       "        -32.837124],\n",
       "       ..., \n",
       "       [-29.092905, -28.669163, -30.504612, ..., -30.011005, -25.868538,\n",
       "        -26.489265],\n",
       "       [-31.591682, -27.878992, -28.669205, ..., -27.512272, -26.817074,\n",
       "        -27.512358],\n",
       "       [-29.092905, -26.814081, -27.154791, ..., -26.817032, -27.881983,\n",
       "        -28.268127]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27193414,  0.29099295,  0.25113701, ...,  0.33259165,\n",
       "         0.30850655,  0.20275585],\n",
       "       [ 0.25113924,  0.27193308,  0.28166051, ...,  0.30850766,\n",
       "         0.25105945,  0.17414523],\n",
       "       [ 0.25113924,  0.29099295,  0.32491869, ...,  0.29091542,\n",
       "         0.27185434,  0.14143428],\n",
       "       ..., \n",
       "       [ 0.23997096,  0.25112259,  0.20281904, ...,  0.21580931,\n",
       "         0.32482668,  0.308491  ],\n",
       "       [ 0.1742106 ,  0.27191753,  0.25112148, ...,  0.28156851,\n",
       "         0.29986404,  0.28156624],\n",
       "       [ 0.23997096,  0.29994281,  0.29097634, ...,  0.29986515,\n",
       "         0.27183882,  0.26167666]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#img_array_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['band_1'] = [(np.array(img).reshape(75,75) - \\\n",
    "                    np.array(img).reshape(75,75).min())/(np.array(img).reshape(75,75).max() - \\\n",
    "                                                         np.array(img).reshape(75,75).min()) \\\n",
    "                   for img in train['band_1']]\n",
    "#img_array_norm = (img_array - img_array.min()) / (img_array.max() - img_array.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "#x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "#X_train = np.concatenate([x_band1[:, :, :, np.newaxis], x_band2[:, :, :, np.newaxis]], axis=-1)\n",
    "y_train_val = np.array(labels, dtype = np.float32).reshape(train.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(test[:1].band_1[0]) == 75**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing = x_test_band1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly put together the TT split code below into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "validation_indices = np.random.choice(x_band1.shape[0], 104, replace = False)\n",
    "training_indices = list(set(np.arange(x_band1.shape[0])) - set(validation_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = x_band1[training_indices, ]\n",
    "validation = x_band1[validation_indices, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(labels[training_indices], dtype = np.float32).reshape(-1, 1)\n",
    "y_validation = np.array(labels[validation_indices], dtype = np.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(sum, y_validation)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5 #+20 # total epochs so far\n",
    "batch_size = 10\n",
    "restore_checkpoint = True #False to start from scratch\n",
    "\n",
    "n_iterations_per_epoch = training.shape[0] // batch_size\n",
    "n_iterations_validation = validation.shape[0] // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./faster_lr_saves/my_faster_capsule_network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./faster_lr_saves/my_faster_capsule_network\n",
      "Epoch: 1  Val accuracy: 66.0000%  Loss: 0.423174 (improved)\n",
      "Epoch: 2  Val accuracy: 68.0000%  Loss: 0.418249 (improved)\n",
      "Epoch: 3  Val accuracy: 70.0000%  Loss: 0.411471 (improved)\n",
      "Epoch: 4  Val accuracy: 70.0000%  Loss: 0.408053 (improved)\n",
      "Epoch: 5  Val accuracy: 71.0000%  Loss: 0.404427 (improved)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    \n",
    "    avg_loss_adamoptimizer_0001_ = np.zeros(n_epochs)\n",
    "    avg_acc_adamoptimizer_0001_ = np.zeros(n_epochs)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch = training[(batch_size*(iteration-1)):(batch_size*iteration), ]\n",
    "            y_batch = y_train[(batch_size*(iteration-1)):(batch_size*iteration), ]\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, 75, 75, 1]),\n",
    "                           y: y_batch.reshape(y_batch.shape[0]),\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch = validation[(batch_size*(iteration-1)):(batch_size*iteration),]\n",
    "            y_batch = y_validation[(batch_size*(iteration-1)):(batch_size*iteration),]\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, 75, 75, 1]),\n",
    "                               y: y_batch.reshape(y_batch.shape[0])})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        avg_loss_adamoptimizer_0001_[epoch] = loss_val\n",
    "        avg_acc_adamoptimizer_0001_[epoch] = acc_val\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved:\n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss_adamoptimizer_0001 = np.append(avg_loss_adamoptimizer_0001, avg_loss_adamoptimizer_0001_)\n",
    "avg_acc_adamoptimizer_0001 = np.append(avg_acc_adamoptimizer_0001, avg_acc_adamoptimizer_0001_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(avg_loss_adamoptimizer_0001, open('avg_loss_adamoptimizer_0001.dill', 'wb'))\n",
    "dill.dump(avg_acc_adamoptimizer_0001, open('avg_acc_adamoptimizer_0001.dill', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss_adamoptimizer_0001 = dill.load(open('avg_loss_adamoptimizer_0001.dill', 'rb'))\n",
    "avg_acc_adamoptimizer_0001 = dill.load(open('avg_acc_adamoptimizer_0001.dill', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x214661fc470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(avg_loss_adamoptimizer_0001, color='blue', alpha=0.8, label='Validation Loss')\n",
    "plt.plot(avg_acc_adamoptimizer_0001, color='green', alpha=0.8, label='Validation Accuracy')\n",
    "#plt.plot(avg_loss_adam, color='black', alpha=0.8, label='Adam')\n",
    "plt.title(\"Validation Loss and Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "#plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations_test = testing.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch = testing[(batch_size*(iteration-1)):(batch_size*iteration), ]\n",
    "        y_batch = y_train[(batch_size*(iteration-1)):(batch_size*iteration), ]\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach 99.43% accuracy on the test set. Pretty nice. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions! We first fix a few images from the test set, then we start a session, restore the trained model, evaluate `caps2_output` to get the capsule network's output vectors, `decoder_output` to get the reconstructions, and `y_pred` to get the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "\n",
    "sample_images = mnist.test.images[:n_samples].reshape([-1, 28, 28, 1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    caps2_output_value, decoder_output_value, y_pred_value = sess.run(\n",
    "            [caps2_output, decoder_output, y_pred],\n",
    "            feed_dict={X: sample_images,\n",
    "                       y: np.array([], dtype=np.int64)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we feed `y` with an empty array, but TensorFlow will not use it, as explained earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's plot the images and their labels, followed by the corresponding reconstructions and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = sample_images.reshape(-1, 28, 28)\n",
    "reconstructions = decoder_output_value.reshape([-1, 28, 28])\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(sample_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(mnist.test.labels[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.title(\"Predicted:\" + str(y_pred_value[index]))\n",
    "    plt.imshow(reconstructions[index], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are all correct, and the reconstructions look great. Hurray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the Output Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tweak the output vectors to see what their pose parameters represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check the shape of the `cap2_output_value` NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that will tweak each of the 16 pose parameters (dimensions) in all output vectors. Each tweaked output vector will be identical to the original output vector, except that one of its pose parameters will be incremented by a value varying from -0.5 to 0.5. By default there will be 11 steps (-0.5, -0.4, ..., +0.4, +0.5). This function will return an array of shape (_tweaked pose parameters_=16, _steps_=11, _batch size_=5, 1, 10, 16, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweak_pose_parameters(output_vectors, min=-0.5, max=0.5, n_steps=11):\n",
    "    steps = np.linspace(min, max, n_steps) # -0.25, -0.15, ..., +0.25\n",
    "    pose_parameters = np.arange(caps2_n_dims) # 0, 1, ..., 15\n",
    "    tweaks = np.zeros([caps2_n_dims, n_steps, 1, 1, 1, caps2_n_dims, 1])\n",
    "    tweaks[pose_parameters, :, 0, 0, 0, pose_parameters, 0] = steps\n",
    "    output_vectors_expanded = output_vectors[np.newaxis, np.newaxis]\n",
    "    return tweaks + output_vectors_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute all the tweaked output vectors and reshape the result to (_parameters_×_steps_×_instances_, 1, 10, 16, 1) so we can feed the array to the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 11\n",
    "\n",
    "tweaked_vectors = tweak_pose_parameters(caps2_output_value, n_steps=n_steps)\n",
    "tweaked_vectors_reshaped = tweaked_vectors.reshape(\n",
    "    [-1, 1, caps2_n_caps, caps2_n_dims, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed these tweaked output vectors to the decoder and get the reconstructions it produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweak_labels = np.tile(mnist.test.labels[:n_samples], caps2_n_dims * n_steps)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    decoder_output_value = sess.run(\n",
    "            decoder_output,\n",
    "            feed_dict={caps2_output: tweaked_vectors_reshaped,\n",
    "                       mask_with_labels: True,\n",
    "                       y: tweak_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reshape the decoder's output so we can easily iterate on the output dimension, the tweak steps, and the instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweak_reconstructions = decoder_output_value.reshape(\n",
    "        [caps2_n_dims, n_steps, n_samples, 28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's plot all the reconstructions, for the first 3 output dimensions, for each tweaking step (column) and each digit (row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(3):\n",
    "    print(\"Tweaking output dimension #{}\".format(dim))\n",
    "    plt.figure(figsize=(n_steps / 1.2, n_samples / 1.5))\n",
    "    for row in range(n_samples):\n",
    "        for col in range(n_steps):\n",
    "            plt.subplot(n_samples, n_steps, row * n_steps + col + 1)\n",
    "            plt.imshow(tweak_reconstructions[dim, col, row], cmap=\"binary\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to make the code in this notebook as flat and linear as possible, to make it easier to follow, but of course in practice you would want to wrap the code in nice reusable functions and classes. For example, you could try implementing your own `PrimaryCapsuleLayer`, and `DenseRoutingCapsuleLayer` classes, with parameters for the number of capsules, the number of routing iterations, whether to use a dynamic loop or a static loop, and so on. For an example a modular implementation of Capsule Networks based on TensorFlow, take a look at the [CapsNet-TensorFlow](https://github.com/naturomics/CapsNet-Tensorflow) project.\n",
    "\n",
    "That's all for today, I hope you enjoyed this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
